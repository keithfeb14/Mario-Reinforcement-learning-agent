{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","authorship_tag":"ABX9TyOTYiGG9hsdC0B6p/S42YAZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-NB-uaIJi9h3","executionInfo":{"status":"ok","timestamp":1683304205864,"user_tz":-60,"elapsed":3482,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}},"outputId":"72ae677f-fd20-4a0b-a1ea-273431d0cc1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym-super-mario-bros==7.4.0 in /usr/local/lib/python3.10/dist-packages (7.4.0)\n","Requirement already satisfied: nes-py>=8.1.4 in /usr/local/lib/python3.10/dist-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n","Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n","Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.65.0)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.22.4)\n","Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n"]}],"source":["#import external packages\n","!pip install gym-super-mario-bros==7.4.0"]},{"cell_type":"code","source":["#import libraries and mount to google drive\n","from google.colab import drive\n","from pathlib import Path\n","\n","import torch\n","from torch import nn\n","import torch\n","from torch import nn\n","from torchvision import transforms as T\n","from PIL import Image\n","import numpy as np\n","from pathlib import Path\n","from collections import deque\n","import random, datetime, os, copy\n","import gym\n","from gym.spaces import Box\n","from gym.wrappers import FrameStack\n","from nes_py.wrappers import JoypadSpace\n","import gym_super_mario_bros\n","\n","from torch.nn.modules.module import register_module_forward_pre_hook\n","from torch.nn.modules.module import register_module_forward_pre_hook\n","from gym.wrappers.monitoring.video_recorder import VideoRecorder\n","\n","import matplotlib.pyplot as plt\n","\n","drive.mount('/content/gdrive',force_remount= True)\n","\n","#initialise the environment\n","env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n","\n","#limit the action space to moving right and right with a jump\n","env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n","env.reset()\n","\n","#observe original state space\n","next_state,reward,done,_,info = env.step(action = 0)\n","print(str(next_state.shape)+'\\n'+str(reward)+'\\n'+str(done)+'\\n'+str(info))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Gz1Gt_CjIc0","executionInfo":{"status":"ok","timestamp":1683304210621,"user_tz":-60,"elapsed":4769,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}},"outputId":"801ebd2b-6287-4a2d-b3b1-fee754bd5a13"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n","(240, 256, 3)\n","0.0\n","False\n","{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"]}]},{"cell_type":"code","source":["#pre-process the environment using reduction of state space. Adapted from https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html\n","class SkipFrame(gym.Wrapper):\n","    def __init__(self, env, skip):\n","        \"\"\"Return only every `skip`-th frame\"\"\"\n","        super().__init__(env)\n","        self._skip = skip\n","\n","    def step(self, action):\n","        \"\"\"Repeat action, and sum reward\"\"\"\n","        total_reward = 0.0\n","        for i in range(self._skip):\n","            # Accumulate reward and repeat the same action\n","            obs, reward, done, trunk, info = self.env.step(action)\n","            total_reward += reward\n","            if done:\n","                break\n","        return obs, total_reward, done, trunk, info\n","\n","\n","class GrayScaleObservation(gym.ObservationWrapper):\n","    def __init__(self, env):\n","        super().__init__(env)\n","        obs_shape = self.observation_space.shape[:2]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def permute_orientation(self, observation):\n","        # permute [H, W, C] array to [C, H, W] tensor\n","        observation = np.transpose(observation, (2, 0, 1))\n","        observation = torch.tensor(observation.copy(), dtype=torch.float)\n","        return observation\n","\n","    def observation(self, observation):\n","        observation = self.permute_orientation(observation)\n","        transform = T.Grayscale()\n","        observation = transform(observation)\n","        return observation\n","\n","\n","class ResizeObservation(gym.ObservationWrapper):\n","    def __init__(self, env, shape):\n","        super().__init__(env)\n","        if isinstance(shape, int):\n","            self.shape = (shape, shape)\n","        else:\n","            self.shape = tuple(shape)\n","\n","        obs_shape = self.shape + self.observation_space.shape[2:]\n","        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n","\n","    def observation(self, observation):\n","        transforms = T.Compose(\n","            [T.Resize(self.shape), T.Normalize(0, 255)]\n","        )\n","        observation = transforms(observation).squeeze(0)\n","        return observation\n","\n","\n","# Apply Wrappers to environment\n","env = SkipFrame(env, skip=4)\n","env = GrayScaleObservation(env)\n","env = ResizeObservation(env, shape=84)\n","if gym.__version__ < '0.26':\n","    env = FrameStack(env, num_stack=4, new_step_api=True)\n","else:\n","    env = FrameStack(env, num_stack=4)"],"metadata":{"id":"nv5CgFb2ktPi","executionInfo":{"status":"ok","timestamp":1683304210622,"user_tz":-60,"elapsed":5,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#define the classes for the neural networks\n","\n","class DuellingNet(nn.Module):\n","\n","    def __init__(self, input_dim, output_dim):\n","        super().__init__()\n","        c, h, w = input_dim\n","\n","        self.feature = nn.Sequential(\n","            nn.Conv2d(c, 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU()\n","        )\n","        \n","        self.advantage = nn.Sequential(\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, output_dim)\n","        )\n","        \n","        self.value = nn.Sequential(\n","            nn.Linear(512, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","\n","        self.online = nn.ModuleDict({\n","            'feature': self.feature,\n","            'advantage': self.advantage,\n","            'value': self.value\n","        })\n","\n","        self.target = copy.deepcopy(self.online)\n","\n","    def forward(self,input,model):\n","        if model == \"online\":\n","            feature = self.online['feature'](input)\n","            advantage = self.online['advantage'](feature)\n","            value = self.online['value'](feature)\n","            q = value + advantage - advantage.mean(dim=1, keepdim=True)\n","\n","            return q\n","\n","        elif model == \"target\":\n","            feature = self.target['feature'](input)\n","            advantage = self.target['advantage'](feature)\n","            value = self.target['value'](feature)\n","            q = value + advantage - advantage.mean(dim=1, keepdim=True)\n","            return q\n","\n","class DoubleNet(nn.Module):\n","    def __init__(self,input_dim, output_dim):\n","        super().__init__()\n","        c, h, w = input_dim\n","        self.online = nn.Sequential(\n","            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(3136, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, output_dim),\n","        )\n","\n","        self.target = copy.deepcopy(self.online)\n","\n","        # Q_target parameters are frozen.\n","        for p in self.target.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, input, model):\n","        if model == \"online\":\n","            return self.online(input)\n","        elif model == \"target\":\n","            return self.target(input)\n"],"metadata":{"id":"iyWORcyXvjYw","executionInfo":{"status":"ok","timestamp":1683304214264,"user_tz":-60,"elapsed":10,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["class Mario_Agent():\n","  #initialise the agent with the corresponding network based on the arguments passed\n","  def __init__(self,save_dir,duelling,random,env):\n","    \n","    #state and action dimensions\n","    self.state_dim = (4,84,84)\n","    self.environment = env\n","    self.action_dim = env.action_space.n\n","    self.save_dir = save_dir\n","\n","    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    #choice of network\n","    if duelling:\n","      print(\"duelling activated\")\n","      self.nn = DuellingNet(self.state_dim, self.action_dim).float()\n","      self.nn.to(device = self.device)\n","\n","    else:\n","      self.nn = DoubleNet(self.state_dim, self.action_dim).float()\n","      self.nn = self.nn.to(device=self.device)\n","\n","    #RL parameters\n","    self.random = random\n","    self.exploration_rate = 1\n","    self.exploration_rate_decay = 0.99\n","    self.exploration_rate_min = 0.1\n","    self.curr_step = 0\n","    self.duelling = duelling\n","    self.save_every = 5e5\n","    self.memory = deque(maxlen=100000)\n","    self.batch_size = 32\n","    self.gamma = 0.9\n","    self.optimizer = torch.optim.Adam(self.nn.parameters(), lr=0.00025)\n","    self.loss = torch.nn.SmoothL1Loss()\n","    self.burnin = 1e4 \n","    self.learn_every = 3 \n","    self.synch_every = 1e4\n","\n","  #action selection based on epsilon greedy\n","  def choose_action(self,state):\n","    if(np.random.rand()<self.exploration_rate):\n","      action_choice = np.random.randint(self.action_dim)\n","    \n","    else:\n","      state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n","      state = torch.tensor(state, device=self.device).unsqueeze(0)\n","      action_values = self.nn(state, model=\"online\")\n","      action_choice = torch.argmax(action_values, axis=1).item()\n","\n","    if(not self.random):\n","      # decrease exploration_rate only if random policy is not enacted\n","      self.exploration_rate *= self.exploration_rate_decay\n","      self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n","\n","    # increment step\n","    self.curr_step += 1\n","    return action_choice\n","\n","  #store experience to memory buffer\n","  def store_experience(self,state,next_state,action,reward,done):\n","    def first_if_tuple(x):\n","      return x[0] if isinstance(x, tuple) else x\n","    state = first_if_tuple(state).__array__()\n","    next_state = first_if_tuple(next_state).__array__()\n","\n","    state = torch.tensor(state, device=self.device)\n","    next_state = torch.tensor(next_state, device=self.device)\n","    action = torch.tensor([action], device=self.device)\n","    reward = torch.tensor([reward], device=self.device)\n","    done = torch.tensor([done], device=self.device)\n","\n","    self.memory.append((state, next_state, action, reward, done,))\n","\n","\n","  def sample_experience(self):\n","    batch = random.sample(self.memory, self.batch_size)\n","    state, next_state, action, reward, done = map(torch.stack, zip(*batch))\n","    return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n","\n","  def update_Q_online(self, td_estimate, td_target):\n","        loss = self.loss(td_estimate, td_target)\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss.item()\n","\n","  def td_estimate(self, state, action):\n","        current_Q = self.nn(state, model=\"online\")[\n","            np.arange(0, self.batch_size), action\n","        ]  # Q_online(s,a)\n","        return current_Q\n","\n","  @torch.no_grad()\n","  def td_target(self, reward, next_state, done):\n","    next_state_Q = self.nn(next_state, model=\"online\")\n","    best_action = torch.argmax(next_state_Q, axis=1)\n","    next_Q = self.nn(next_state, model=\"target\")[\n","    np.arange(0, self.batch_size), best_action]\n","    return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n","\n","  def experience_replay(self):\n","\n","    #synch the target and the online networks\n","    if self.curr_step % self.synch_every == 0:\n","      self.nn.target.load_state_dict(self.nn.online.state_dict())\n","\n","    #save to folder every save steps\n","    if self.curr_step % self.save_every == 0:\n","      self.save()\n","\n","    #if the agent has not reached the burn in period return\n","    if self.curr_step<self.burnin:\n","      return\n","\n","    #batch size can never be greater than the size of the buffer\n","    if self.batch_size > len(self.memory):\n","      return\n","\n","        \n","    if self.curr_step % self.learn_every != 0:\n","      return\n","\n","    # Sample from memory\n","    state, next_state, action, reward, done = self.sample_experience()\n","\n","    # Get TD estimate\n","    td_est = self.td_estimate(state, action)\n","\n","    # Get TD Target\n","    td_tgt = self.td_target(reward, next_state, done)\n","\n","    # Backpropagate loss through Q_online\n","    self.update_Q_online(td_est, td_tgt)\n","    return\n","\n","  #save the agent path at certain amount of time \n","  def save(self):\n","        if(self.duelling):\n","          save_path = (\n","              Path(self.save_dir) / f\"mario_duelling_{int(self.curr_step // self.save_every)}.chkpt\"\n","          )\n","          torch.save(\n","              dict(model=self.nn.state_dict(), exploration_rate=self.exploration_rate),\n","              save_path,\n","          )\n","        elif(not self.duelling):\n","          save_path = (\n","              Path(self.save_dir) / f\"mario_ddqn_{int(self.curr_step // self.save_every)}.chkpt\"\n","          )\n","          torch.save(\n","              dict(model=self.nn.state_dict(), exploration_rate=self.exploration_rate),\n","              save_path,\n","          )\n","        else:\n","          print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n","\n","\n","  def run_agent(self,number_of_episodes):\n","    episode_rewards = []\n","    average_rewards = []\n","    epsilon = []\n","\n","    for e in range(number_of_episodes):\n","      e_reward = 0\n","      state = self.environment.reset()\n","      while True:\n","        #choose and take action\n","        action = self.choose_action(state)\n","\n","        next_state, reward, done, trunc, info = self.environment.step(action)\n","        \n","        #store the reward in memory\n","        e_reward+=reward\n","        self.store_experience(state,next_state,action,reward,done)\n","\n","        #learn\n","        self.experience_replay()\n","\n","        #update the state\n","        state = next_state\n","\n","        if done or info['flag_get']:\n","          break\n","    \n","      epsilon.append(self.exploration_rate)\n","      episode_rewards.append(e_reward)\n","\n","      if e%10 == 0:\n","        average_rewards.append(np.mean(episode_rewards[-10:]))\n","    return((episode_rewards,average_rewards,epsilon))"],"metadata":{"id":"-a1hO4J_mykS","executionInfo":{"status":"ok","timestamp":1683305214503,"user_tz":-60,"elapsed":533,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"833a2e27-c581-4e3f-d7b9-2fc0fdda7978"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["#running episodes is an expensive process so it is better to have initial plots in case the run time stops\n","def plot_learnings(thing,title,other_thing):\n","\n","  fig, ax = plt.subplots(figsize=(6,4))\n","  x = np.arange(len(thing))\n","  ax.set_title(title)\n","  ax.set_xlabel('episode number')\n","  ax.set_ylabel(other_thing)\n","  ax.plot(x,thing)\n","  fig.tight_layout()"],"metadata":{"id":"gFd6QOgsQUlW","executionInfo":{"status":"ok","timestamp":1683304962570,"user_tz":-60,"elapsed":823,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def record_video(checkpoint,file_name,duelling,random):\n","  #record a video for a particular checkpoint (please load the appropriate files)\n","\n","  mario = Mario_Agent(None,duelling,random,env)\n","\n","  # Load the checkpoint file\n","  checkpoint_path = F\"/content/gdrive/MyDrive/Colab Notebooks/\"+checkpoint\n","  checkpoint = torch.load(checkpoint_path)\n","  mario.nn.load_state_dict(checkpoint['model'])\n","  mario.exploration_rate = checkpoint['exploration_rate']\n","\n","  # Start recording the video\n","  video_path = '/content/gdrive/MyDrive/Colab Notebooks/'+file_name +\".mp4\"\n","  video_recorder = VideoRecorder(env, video_path)\n","\n","  # Play the game!\n","  state = env.reset()\n","  while True:\n","    # Run agent on the state\n","    action = mario.choose_action(state)\n","\n","    # Agent performs action\n","    next_state, reward, done, trunc, info = env.step(action)\n","\n","    # Record the video\n","    video_recorder.capture_frame()\n","\n","    # Update state\n","    state = next_state\n","\n","    # Check if end of game\n","    if done or info[\"flag_get\"]:\n","        break\n","\n","  # Stop recording the video\n","  video_recorder.close()\n"],"metadata":{"id":"8Y_F9lDYc8km","executionInfo":{"status":"ok","timestamp":1683304963184,"user_tz":-60,"elapsed":2,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# #create the agent ddqn\n","save_dir = F\"/content/gdrive/MyDrive/Colab Notebooks/\"\n","\n","# #ddqn mario\n","mario_ddqn = Mario_Agent(save_dir,False,False,env)\n","rewards1,average_rewards1,epsilon1 = mario_ddqn.run_agent(100)\n","plot_learnings(rewards1,\"DDQN rewards\",'reward')"],"metadata":{"id":"m9oB-8BcC92-","executionInfo":{"status":"error","timestamp":1683305326726,"user_tz":-60,"elapsed":107191,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"4362c8d8-660a-4030-c3a5-a4d4787ebd96"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n","I MADE IT\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-628b6e9b50ad>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# #ddqn mario\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmario_ddqn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMario_Agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mrewards1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage_rewards1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmario_ddqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplot_learnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"DDQN rewards\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-b7e686279dc7>\u001b[0m in \u001b[0;36mrun_agent\u001b[0;34m(self, number_of_episodes)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m#store the reward in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/frame_stack.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \"\"\"\n\u001b[1;32m    176\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         )\n\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 483\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    484\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_returns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-20-96f293e55d9c>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_skip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# Accumulate reward and repeat the same action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mtotal_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nes_py/wrappers/joypad_space.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \"\"\"\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# take the step and record the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_action_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \"\"\"\n\u001b[1;32m     59\u001b[0m         observation, reward, terminated, truncated, info = step_api_compatibility(\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/order_enforcing.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_reset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mResetNeeded\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot call env.step() before calling env.reset()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \"\"\"\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mstep_returns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_step_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstep_to_new_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_returns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/wrappers/env_checker.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0menv_step_passive_checker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nes_py/nes_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrollers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;31m# pass the action to the emulator as an unsigned byte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;31m# get the reward for this step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["#create the agent duelling\n","\n","#duelling mario\n","mario_duelling = Mario_Agent(save_dir,True,False,env)\n","rewards2,average_rewards2,epsilon2 = mario_ddqn.run_agent(10000)\n","\n","plot_learnings(rewards2,\"Duelling DDQN rewards\",'reward')"],"metadata":{"id":"idFOhqCrO4Tk","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"ok","timestamp":1683300947846,"user_tz":-60,"elapsed":169441,"user":{"displayName":"Xander Ito-Low","userId":"01161905731820319898"}},"outputId":"135d0d3a-b832-4c3a-f455-3c573fae4c42"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["duelling activated\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 600x400 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["#create the agent random\n","\n","#random mario\n","mario_random = Mario_Agent(save_dir,True,True,env)\n","rewards3,average_rewards3,epsilon3 = mario_ddqn.run_agent(100)\n","\n","fig, ax = plt.subplots(figsize=(6,4))\n","x = np.arange(len(average_rewards3))\n","ax.set_title('10 episode average over 100 episodes')\n","ax.set_xlabel('episode number')\n","ax.set_ylabel('average reward')\n","ax.plot(x,average_rewards3,label = \"Purely random actions\")\n","ax.legend(loc='upper left')\n","fig.tight_layout()\n","plt.show()"],"metadata":{"id":"vn81IzBxRGc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig, ax = plt.subplots(figsize=(6,4))\n","x = np.arange(len(average_rewards1))\n","ax.set_title('Comparison of DDQN and Duelling DQN using average of 10 episodes')\n","ax.set_xlabel('episode number')\n","ax.set_ylabel('average reward')\n","ax.plot(x,average_rewards1,label = \"DDQN\")\n","ax.plot(x,average_rewards2,label = \"Duelling DQN\")\n","ax.legend(loc='upper left')\n","fig.tight_layout()"],"metadata":{"id":"G49mxCNjQRFl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#now record resulting video for whatever checkpoints you have saved for any of the agent. This is prone to crashing so we get our outputs then manually generate the videos from the output\n","record_video(\"mario_ddqn_3.chkpt\",\"Double\",False,False)"],"metadata":{"id":"mjcYV4psgv71"},"execution_count":null,"outputs":[]}]}